{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import cv2\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_guard_api_key():\n",
    "    load_env()\n",
    "    PREDICTION_GUARD_API_KEY = os.getenv(\"PREDICTION_GUARD_API_KEY\", None)\n",
    "    if PREDICTION_GUARD_API_KEY is None:\n",
    "        PREDICTION_GUARD_API_KEY = input(\"Please enter your Prediction Guard API Key: \")\n",
    "    return PREDICTION_GUARD_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_GUARD_URL_ENDPOINT = os.getenv(\"DLAI_PREDICTION_GUARD_URL_ENDPOINT\", \"https://dl-itdc.predictionguard.com\") ###\"https://proxy-dl-itdc.predictionguard.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to compute the joint embedding of a prompt and a base64-encoded image through PredictionGuard\n",
    "def bt_embedding_from_prediction_guard(prompt, base64_image):\n",
    "    # get PredictionGuard client\n",
    "    client = _getPredictionGuardClient()\n",
    "    message = {\"text\": prompt,}\n",
    "    if base64_image is not None and base64_image != \"\":\n",
    "        if not isBase64(base64_image): \n",
    "            raise TypeError(\"image input must be in base64 encoding!\")\n",
    "        message['image'] = base64_image\n",
    "    response = client.embeddings.create(\n",
    "        model=\"bridgetower-large-itm-mlm-itc\",\n",
    "        input=[message]\n",
    "    )\n",
    "    return response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding image at given path or PIL Image using base64\n",
    "def encode_image(image_path_or_PIL_img):\n",
    "    if isinstance(image_path_or_PIL_img, PIL.Image.Image):\n",
    "        # this is a PIL image\n",
    "        buffered = BytesIO()\n",
    "        image_path_or_PIL_img.save(buffered, format=\"JPEG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    else:\n",
    "        # this is a image_path\n",
    "        with open(image_path_or_PIL_img, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use your own uploaded images and captions. \n",
    "# You will be responsible for the legal use of images that \n",
    "#  you are going to use.\n",
    "\n",
    "url1='http://farm3.staticflickr.com/2519/4126738647_cc436c111b_z.jpg'\n",
    "cap1='A motorcycle sits parked across from a herd of livestock'\n",
    "\n",
    "url2='http://farm3.staticflickr.com/2046/2003879022_1b4b466d1d_z.jpg'\n",
    "cap2='Motorcycle on a platform to be worked on in garage'\n",
    "\n",
    "url3='http://farm1.staticflickr.com/133/356148800_9bf03b6116_z.jpg'\n",
    "cap3='a cat lying down stretched out near a laptop'\n",
    "\n",
    "img1 = {\n",
    "  'flickr_url': url1,\n",
    "  'caption': cap1,\n",
    "  'image_path' : './shared_data/motorcycle_1.jpg'\n",
    "}\n",
    "\n",
    "img2 = {\n",
    "    'flickr_url': url2,\n",
    "    'caption': cap2,\n",
    "    'image_path' : './shared_data/motorcycle_2.jpg'\n",
    "}\n",
    "\n",
    "img3 = {\n",
    "    'flickr_url' : url3,\n",
    "    'caption': cap3,\n",
    "    'image_path' : './shared_data/cat_1.jpg'\n",
    "}\n",
    "\n",
    "# download images\n",
    "imgs = [img1, img2, img3]\n",
    "for img in imgs:\n",
    "    data = requests.get(img['flickr_url']).content\n",
    "    with open(img['image_path'], 'wb') as f:\n",
    "        f.write(data)\n",
    "\n",
    "for img in [img1, img2, img3]:\n",
    "    image = Image.open(img['image_path'])\n",
    "    caption = img['caption']\n",
    "    display(image)\n",
    "    display(caption)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for img in [img1, img2, img3]:\n",
    "    img_path = img['image_path']\n",
    "    caption = img['caption']\n",
    "    base64_img = encode_image(img_path)\n",
    "    embedding = bt_embeddings(caption, base64_img)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image-text pair is now converted into multimodal \n",
    "# embedding vector which has dimensions of 512.\n",
    "\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    similarity = np.dot(vec1,vec2)/(norm(vec1)*norm(vec2))\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_embed = np.array(embeddings[0])\n",
    "ex2_embed = np.array(embeddings[1])\n",
    "ex3_embed = np.array(embeddings[2])\n",
    "sim_ex1_ex2 = cosine_similarity(ex1_embed, ex2_embed)\n",
    "sim_ex2_ex3 = cosine_similarity(ex2_embed, ex3_embed)\n",
    "sim_ex1_ex3 = cosine_similarity(ex1_embed, ex3_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine similarity between ex1_embeded and ex2_embeded is:\")\n",
    "display(sim_ex1_ex2)\n",
    "print(\"Cosine similarity between ex2_embeded and ex3_embeded is:\")\n",
    "display(sim_ex2_ex3)\n",
    "print(\"Cosine similarity between ex1_embeded and ex3_embeded is:\")\n",
    "display(sim_ex1_ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function helps to prepare list image-text pairs from the first [test_size] data of a Huggingface dataset\n",
    "def prepare_dataset_for_umap_visualization(hf_dataset, class_name, templates=templates, test_size=1000):\n",
    "    # load Huggingface dataset (download if needed)\n",
    "    dataset = load_dataset(hf_dataset, trust_remote_code=True)\n",
    "    # split dataset with specific test_size\n",
    "    train_test_dataset = dataset['train'].train_test_split(test_size=test_size)\n",
    "    # get the test dataset\n",
    "    test_dataset = train_test_dataset['test']\n",
    "    img_txt_pairs = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        img_txt_pairs.append({\n",
    "            'caption' : templates[random.randint(0, len(templates)-1)].format(class_name),\n",
    "            'pil_img' : test_dataset[i]['image']\n",
    "        })\n",
    "    return img_txt_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 50 data of Huggingface dataset \n",
    "#  \"yashikota/cat-image-dataset\"\n",
    "cat_img_txt_pairs = data_prep(\"yashikota/cat-image-dataset\", \n",
    "                             \"cat\", test_size=50)\n",
    "\n",
    "# for the first 50 data of Huggingface dataset \n",
    "#  \"tanganke/stanford_cars\"\n",
    "car_img_txt_pairs = data_prep(\"tanganke/stanford_cars\", \n",
    "                             \"car\", test_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an example of a cat image-text pair data\n",
    "display(cat_img_txt_pairs[0]['caption'])\n",
    "display(cat_img_txt_pairs[0]['pil_img'])\n",
    "\n",
    "# display an example of a car image-text pair data\n",
    "display(car_img_txt_pairs[0]['caption'])\n",
    "display(car_img_txt_pairs[0]['pil_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BridgeTower embeddings for cat image-text pairs\n",
    "cat_embeddings = []\n",
    "for img_txt_pair in tqdm(\n",
    "                        cat_img_txt_pairs, \n",
    "                        total=len(cat_img_txt_pairs)\n",
    "                    ):\n",
    "    pil_img = img_txt_pair['pil_img']\n",
    "    caption = img_txt_pair['caption']\n",
    "    base64_img = encode_image(pil_img)\n",
    "    embedding = bt_embeddings(caption, base64_img)\n",
    "    cat_embeddings.append(embedding)\n",
    "\n",
    "# compute BridgeTower embeddings for car image-text pairs\n",
    "car_embeddings = []\n",
    "for img_txt_pair in tqdm(\n",
    "                        car_img_txt_pairs, \n",
    "                        total=len(car_img_txt_pairs)\n",
    "                    ):\n",
    "    pil_img = img_txt_pair['pil_img']\n",
    "    caption = img_txt_pair['caption']\n",
    "    base64_img = encode_image(pil_img)\n",
    "    embedding = bt_embeddings(caption, base64_img)\n",
    "    car_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function transforms high-dimension vectors to 2D vectors using UMAP\n",
    "def dimensionality_reduction(embed_arr, label):\n",
    "    X_scaled = MinMaxScaler().fit_transform(embed_arr)\n",
    "    print(X_scaled)\n",
    "    mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "    df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "    df_emb[\"label\"] = label\n",
    "    print(df_emb)\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking embeddings of cat and car examples into one numpy array\n",
    "all_embeddings = np.concatenate([cat_embeddings, car_embeddings])\n",
    "\n",
    "# prepare labels \n",
    "labels = ['cat'] * len(cat_embeddings) + ['car'] * len(car_embeddings)\n",
    "\n",
    "# compute dimensionality reduction \n",
    "reduced_dim_emb = dimensionality_reduction(all_embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the centroids against the cluster\n",
    "fig, ax = plt.subplots(figsize=(8,6)) # Set figsize\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.scatterplot(data=reduced_dim_emb, \n",
    "                x=reduced_dim_emb['X'], \n",
    "                y=reduced_dim_emb['Y'], \n",
    "                hue='label', \n",
    "                palette='bright')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title('Scatter plot of images of cats and cars using UMAP')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
